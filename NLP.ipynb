{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dcd79a",
   "metadata": {},
   "source": [
    "# NLP analysis objectives\n",
    "- switch to outscraper to extract full information of reviews ✓\n",
    "- check if the place might have water army?\n",
    "- extract the most related\n",
    "- use sentiment analysis to analysis the score rather than using google score\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7165a63",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "- download a set of data with only reviews and rating or Aurther ✓\n",
    "- Word2vec to show its true reviews\n",
    "- sentiment analysis to measure its true value\n",
    "## data preprocessing:\n",
    "- stopword ✓\n",
    "- lemmetize ✓\n",
    "- porterstemmer (clean liked like liking into like) ✓\n",
    "- embedding ✓\n",
    "- train test split ✓\n",
    "- word2vec after cleaning ✓\n",
    "- onehotencoding y as well\n",
    "\n",
    "# Tech issue\n",
    "- maybe code a function to download necessary nlp trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57efda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re,string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_references/yelp.csv')\n",
    "df=df[['text','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837d9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(word):\n",
    "    model= stopwords.words('english')\n",
    "    trim_s=WordNetLemmatizer()\n",
    "    stem=PorterStemmer()\n",
    "    word = ''.join([i for i in str(word) if not i.isdigit()])\n",
    "    word = word_tokenize(word.lower())\n",
    "    word = [i for i in set(word) if i.isalpha() and i not in string.punctuation and i not in model]\n",
    "    word = [trim_s.lemmatize(i) for i in word]\n",
    "    word = [stem.stem(i) for i in word]\n",
    "    \n",
    "    return \" \".join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdf4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_review']=np.vectorize(clean_data)(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c8cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['true_review']\n",
    "y=df[['stars']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfd455",
   "metadata": {},
   "source": [
    " # Hyper parameters for grid search:\n",
    "## word2vec\n",
    "- vector_size\n",
    "- min_count\n",
    "- window\n",
    "## padding\n",
    "- maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5663a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 13:32:18.469164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:18.469198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-04 13:32:24.615125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-04 13:32:24.618928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619008: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619112: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:32:24.619273: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-04 13:32:24.620315: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1 out of 24 with 4% progress\n",
      "finished 2 out of 24 with 8% progress\n",
      "finished 3 out of 24 with 12% progress\n",
      "finished 4 out of 24 with 16% progress\n",
      "finished 5 out of 24 with 20% progress\n",
      "finished 6 out of 24 with 25% progress\n",
      "finished 7 out of 24 with 29% progress\n",
      "finished 8 out of 24 with 33% progress\n",
      "finished 9 out of 24 with 37% progress\n",
      "finished 10 out of 24 with 41% progress\n",
      "finished 11 out of 24 with 45% progress\n",
      "finished 12 out of 24 with 50% progress\n",
      "finished 13 out of 24 with 54% progress\n",
      "finished 14 out of 24 with 58% progress\n",
      "finished 15 out of 24 with 62% progress\n",
      "finished 16 out of 24 with 66% progress\n",
      "finished 17 out of 24 with 70% progress\n",
      "finished 18 out of 24 with 75% progress\n",
      "finished 19 out of 24 with 79% progress\n",
      "finished 20 out of 24 with 83% progress\n",
      "finished 21 out of 24 with 87% progress\n",
      "finished 22 out of 24 with 91% progress\n",
      "finished 23 out of 24 with 95% progress\n",
      "finished 24 out of 24 with 100% progress\n"
     ]
    }
   ],
   "source": [
    "vector_size=[50,60,70,80]\n",
    "window=[5,6,7,8,9,10]\n",
    "maxlen=[70]\n",
    "\n",
    "col_vec=[]\n",
    "col_win=[]\n",
    "col_max=[]\n",
    "\n",
    "for a in vector_size:\n",
    "    for b in window:\n",
    "        for c in maxlen:\n",
    "            col_vec.append(a)\n",
    "            col_win.append(b)\n",
    "            col_max.append(c)\n",
    "        \n",
    "\n",
    "df_test=pd.DataFrame({'vector':col_vec,'window':col_win,'maxlen':col_max})\n",
    "hist_list=[]\n",
    "for z in range(0,len(df_test)):\n",
    "    new_y=[]\n",
    "    for i in df.stars:\n",
    "        if i ==5:\n",
    "            new_y.append([0,0,0,0,1])\n",
    "        elif i ==4:\n",
    "            new_y.append([0,0,0,1,0])\n",
    "        elif i ==3:\n",
    "            new_y.append([0,0,1,0,0])\n",
    "        elif i ==2:\n",
    "            new_y.append([0,1,0,0,0])\n",
    "        else:\n",
    "            new_y.append([1,0,0,0,0])\n",
    "    new_y=np.array(new_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.33, random_state=42)\n",
    "\n",
    "    word2vec = Word2Vec(sentences=X_train, size=df_test['vector'][z], min_count=1, window=df_test['window'][z])\n",
    "\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "\n",
    "    # Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "    def embed_sentence(word2vec, sentence):\n",
    "        embedded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec.wv:\n",
    "                embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "        return np.array(embedded_sentence)\n",
    "\n",
    "    # Function that converts a list of sentences into a list of matrices\n",
    "    def embedding(word2vec, sentences):\n",
    "        embed = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "            embed.append(embedded_sentence)\n",
    "\n",
    "        return embed\n",
    "\n",
    "    # Embed the training and test sentences\n",
    "    X_train_embed = embedding(word2vec, X_train)\n",
    "    X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "    X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    def init_model():\n",
    "        model = Sequential()\n",
    "        model.add(layers.Masking())\n",
    "        model.add(layers.LSTM(20, activation='tanh'))\n",
    "        model.add(layers.Dense(15, activation='relu'))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    model=init_model()\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history=model.fit(X_train_pad, y_train, \n",
    "              batch_size = 64,\n",
    "              epochs=500,\n",
    "              callbacks=[es],\n",
    "              validation_data=(X_test_pad,y_test),\n",
    "              verbose=0\n",
    "             )  \n",
    "    hist_list.append(history.history['val_accuracy'][-1])\n",
    "    print(f'finished {z+1} out of {len(df_test)} with {int(((z+1)/len(df_test))*100)}% progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d04da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['val_acc']=hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f733ae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>window</th>\n",
       "      <th>maxlen</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>0.353636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>0.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>0.348788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.357576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.377576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>0.368485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.356667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>0.372727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>0.374848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.363939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.364545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>0.352424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.367273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>0.365152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>0.369394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.359091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.368182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>0.364848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.353333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>0.362727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>0.358788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.371212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vector  window  maxlen   val_acc\n",
       "0       50       5      70  0.363636\n",
       "1       50       6      70  0.353636\n",
       "2       50       7      70  0.363636\n",
       "3       50       8      70  0.362727\n",
       "4       50       9      70  0.348788\n",
       "5       50      10      70  0.357576\n",
       "6       60       5      70  0.377576\n",
       "7       60       6      70  0.368485\n",
       "8       60       7      70  0.356667\n",
       "9       60       8      70  0.372727\n",
       "10      60       9      70  0.374848\n",
       "11      60      10      70  0.363939\n",
       "12      70       5      70  0.364545\n",
       "13      70       6      70  0.352424\n",
       "14      70       7      70  0.367273\n",
       "15      70       8      70  0.365152\n",
       "16      70       9      70  0.369394\n",
       "17      70      10      70  0.359091\n",
       "18      80       5      70  0.368182\n",
       "19      80       6      70  0.364848\n",
       "20      80       7      70  0.353333\n",
       "21      80       8      70  0.362727\n",
       "22      80       9      70  0.358788\n",
       "23      80      10      70  0.371212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6eab94",
   "metadata": {},
   "source": [
    "now need to convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f266a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git add NLP.ipynb\n",
    "!git commit -m 'created function for grid search'\n",
    "!git push origin master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ac7dd",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
