{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dcd79a",
   "metadata": {},
   "source": [
    "# NLP analysis objectives\n",
    "- switch to outscraper to extract full information of reviews ✓\n",
    "- check if the place might have water army?\n",
    "- extract the most related\n",
    "- use sentiment analysis to analysis the score rather than using google score\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7165a63",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "- download a set of data with only reviews and rating or Aurther ✓\n",
    "- Word2vec to show its true reviews\n",
    "- sentiment analysis to measure its true value\n",
    "## data preprocessing:\n",
    "- stopword ✓\n",
    "- lemmetize ✓\n",
    "- porterstemmer (clean liked like liking into like) ✓\n",
    "- embedding ✓\n",
    "- train test split ✓\n",
    "- word2vec after cleaning ✓\n",
    "- onehotencoding y as well\n",
    "\n",
    "# Tech issue\n",
    "- maybe code a function to download necessary nlp trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57efda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re,string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_references/yelp.csv')\n",
    "df=df[['text','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837d9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(word):\n",
    "    model= stopwords.words('english')\n",
    "    trim_s=WordNetLemmatizer()\n",
    "    stem=PorterStemmer()\n",
    "    word = ''.join([i for i in str(word) if not i.isdigit()])\n",
    "    word = word_tokenize(word.lower())\n",
    "    word = [i for i in set(word) if i.isalpha() and i not in string.punctuation and i not in model]\n",
    "    word = [trim_s.lemmatize(i) for i in word]\n",
    "    word = [stem.stem(i) for i in word]\n",
    "    \n",
    "    return \" \".join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdf4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_review']=np.vectorize(clean_data)(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c8cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['true_review']\n",
    "y=df[['stars']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfd455",
   "metadata": {},
   "source": [
    " # Hyper parameters for grid search:\n",
    "## word2vec\n",
    "- vector_size\n",
    "- min_count\n",
    "- window\n",
    "## padding\n",
    "- maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5663a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 22:50:44.033205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:44.033245: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-15 22:50:50.084206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-15 22:50:50.084406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084524: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084651: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-15 22:50:50.084658: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-15 22:50:50.085170: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1 out of 6 with 16% progress\n",
      "finished 2 out of 6 with 33% progress\n",
      "finished 3 out of 6 with 50% progress\n",
      "finished 4 out of 6 with 66% progress\n",
      "finished 5 out of 6 with 83% progress\n"
     ]
    }
   ],
   "source": [
    "vector_size=[50]\n",
    "window=[5,6,7,8,9,10]\n",
    "maxlen=[70]\n",
    "\n",
    "col_vec=[]\n",
    "col_win=[]\n",
    "col_max=[]\n",
    "\n",
    "for a in vector_size:\n",
    "    for b in window:\n",
    "        for c in maxlen:\n",
    "            col_vec.append(a)\n",
    "            col_win.append(b)\n",
    "            col_max.append(c)\n",
    "        \n",
    "\n",
    "df_test=pd.DataFrame({'vector':col_vec,'window':col_win,'maxlen':col_max})\n",
    "hist_list=[]\n",
    "for z in range(0,len(df_test)):\n",
    "    new_y=[]\n",
    "    for i in df.stars:\n",
    "        if i ==5:\n",
    "            new_y.append([0,0,0,0,1])\n",
    "        elif i ==4:\n",
    "            new_y.append([0,0,0,1,0])\n",
    "        elif i ==3:\n",
    "            new_y.append([0,0,1,0,0])\n",
    "        elif i ==2:\n",
    "            new_y.append([0,1,0,0,0])\n",
    "        else:\n",
    "            new_y.append([1,0,0,0,0])\n",
    "    new_y=np.array(new_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.33, random_state=42)\n",
    "\n",
    "    word2vec = Word2Vec(sentences=X_train, vector_size=df_test['vector'][z], min_count=1, window=df_test['window'][z])\n",
    "\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "\n",
    "    # Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "    def embed_sentence(word2vec, sentence):\n",
    "        embedded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec.wv:\n",
    "                embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "        return np.array(embedded_sentence)\n",
    "\n",
    "    # Function that converts a list of sentences into a list of matrices\n",
    "    def embedding(word2vec, sentences):\n",
    "        embed = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "            embed.append(embedded_sentence)\n",
    "\n",
    "        return embed\n",
    "\n",
    "    # Embed the training and test sentences\n",
    "    X_train_embed = embedding(word2vec, X_train)\n",
    "    X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "    X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    def init_model():\n",
    "        model = Sequential()\n",
    "        model.add(layers.Masking())\n",
    "        model.add(layers.LSTM(20, activation='tanh'))\n",
    "        model.add(layers.Dense(15, activation='relu'))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    model=init_model()\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history=model.fit(X_train_pad, y_train, \n",
    "              batch_size = 64,\n",
    "              epochs=500,\n",
    "              callbacks=[es],\n",
    "              validation_data=(X_test_pad,y_test),\n",
    "              verbose=0\n",
    "             )  \n",
    "    hist_list.append(history.history['val_accuracy'][-1])\n",
    "    print(f'finished {z+1} out of {len(df_test)} with {int(((z+1)/len(df_test))*100)}% progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['val_acc']=hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f266a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git add NLP.ipynb\n",
    "!git commit -m 'created function for grid search'\n",
    "!git push origin master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ac7dd",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
