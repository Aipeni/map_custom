{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dcd79a",
   "metadata": {},
   "source": [
    "# NLP analysis objectives\n",
    "- switch to outscraper to extract full information of reviews ✓\n",
    "- check if the place might have water army?\n",
    "- extract the most related\n",
    "- use sentiment analysis to analysis the score rather than using google score\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7165a63",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "- download a set of data with only reviews and rating or Aurther ✓\n",
    "- Word2vec to show its true reviews\n",
    "- sentiment analysis to measure its true value\n",
    "## data preprocessing:\n",
    "- stopword ✓\n",
    "- lemmetize ✓\n",
    "- porterstemmer (clean liked like liking into like) ✓\n",
    "- embedding ✓\n",
    "- train test split ✓\n",
    "- word2vec after cleaning ✓\n",
    "- onehotencoding y as well\n",
    "\n",
    "# Tech issue\n",
    "- maybe code a function to download necessary nlp trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57efda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aipen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re,string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_references/yelp.csv')\n",
    "df=df[['text','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837d9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(word):\n",
    "    model= stopwords.words('english')\n",
    "    trim_s=WordNetLemmatizer()\n",
    "    stem=PorterStemmer()\n",
    "    word = ''.join([i for i in str(word) if not i.isdigit()])\n",
    "    word = word_tokenize(word.lower())\n",
    "    word = [i for i in set(word) if i.isalpha() and i not in string.punctuation and i not in model]\n",
    "    word = [trim_s.lemmatize(i) for i in word]\n",
    "    word = [stem.stem(i) for i in word]\n",
    "    \n",
    "    return \" \".join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdf4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_review']=np.vectorize(clean_data)(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c8cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['true_review']\n",
    "y=df[['stars']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88226b",
   "metadata": {},
   "source": [
    " # Hyper parameters for grid search:\n",
    "## word2vec\n",
    "- vector_size\n",
    "- min_count\n",
    "- window\n",
    "## padding\n",
    "- maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b928b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 1 out of 28 with 3% progress\n",
      "finished 2 out of 28 with 7% progress\n",
      "finished 3 out of 28 with 10% progress\n",
      "finished 4 out of 28 with 14% progress\n",
      "finished 5 out of 28 with 17% progress\n",
      "finished 6 out of 28 with 21% progress\n",
      "finished 7 out of 28 with 25% progress\n",
      "finished 8 out of 28 with 28% progress\n",
      "finished 9 out of 28 with 32% progress\n",
      "finished 10 out of 28 with 35% progress\n",
      "finished 11 out of 28 with 39% progress\n",
      "finished 12 out of 28 with 42% progress\n",
      "finished 13 out of 28 with 46% progress\n"
     ]
    }
   ],
   "source": [
    "vector_size=[50]\n",
    "window=[5,6,7,8]\n",
    "maxlen=[10,20,30,40,50,60,70]\n",
    "\n",
    "col_vec=[]\n",
    "col_win=[]\n",
    "col_max=[]\n",
    "\n",
    "for a in vector_size:\n",
    "    for b in window:\n",
    "        for c in maxlen:\n",
    "            col_vec.append(a)\n",
    "            col_win.append(b)\n",
    "            col_max.append(c)\n",
    "        \n",
    "\n",
    "df_test=pd.DataFrame({'vector':col_vec,'window':col_win,'maxlen':col_max})\n",
    "hist_list=[]\n",
    "for z in range(0,len(df_test)):\n",
    "    new_y=[]\n",
    "    for i in df.stars:\n",
    "        if i ==5:\n",
    "            new_y.append([0,0,0,0,1])\n",
    "        elif i ==4:\n",
    "            new_y.append([0,0,0,1,0])\n",
    "        elif i ==3:\n",
    "            new_y.append([0,0,1,0,0])\n",
    "        elif i ==2:\n",
    "            new_y.append([0,1,0,0,0])\n",
    "        else:\n",
    "            new_y.append([1,0,0,0,0])\n",
    "    new_y=np.array(new_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, new_y, test_size=0.33, random_state=42)\n",
    "\n",
    "    word2vec = Word2Vec(sentences=X_train, vector_size=df_test['vector'][z], min_count=1, window=df_test['window'][z])\n",
    "\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "\n",
    "    # Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "    def embed_sentence(word2vec, sentence):\n",
    "        embedded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec.wv:\n",
    "                embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "        return np.array(embedded_sentence)\n",
    "\n",
    "    # Function that converts a list of sentences into a list of matrices\n",
    "    def embedding(word2vec, sentences):\n",
    "        embed = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "            embed.append(embedded_sentence)\n",
    "\n",
    "        return embed\n",
    "\n",
    "    # Embed the training and test sentences\n",
    "    X_train_embed = embedding(word2vec, X_train)\n",
    "    X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "    X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=df_test['maxlen'][z])\n",
    "\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    def init_model():\n",
    "        model = Sequential()\n",
    "        model.add(layers.Masking())\n",
    "        model.add(layers.LSTM(20, activation='tanh'))\n",
    "        model.add(layers.Dense(15, activation='relu'))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='relu'))\n",
    "        model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    model=init_model()\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history=model.fit(X_train_pad, y_train, \n",
    "              batch_size = 64,\n",
    "              epochs=500,\n",
    "              callbacks=[es],\n",
    "              validation_data=(X_test_pad,y_test),\n",
    "              verbose=0\n",
    "             )  \n",
    "    hist_list.append(history.history['val_accuracy'][-1])\n",
    "    print(f'finished {z+1} out of {len(df_test)} with {int(((z+1)/len(df_test))*100)}% progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf951be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['val_acc']=hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb58ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f266a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!git add NLP.ipynb\\n!git commit -m 'try 500 epoch'\\n!git push origin master\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!git add NLP.ipynb\n",
    "!git commit -m 'create table for grid search'\n",
    "!git push origin master\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c1817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
